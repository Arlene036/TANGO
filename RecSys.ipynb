{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwgDK2OdV2gM",
        "outputId": "77b6a445-7423-4483-9c21-a8558dd95f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EwKpZtj8VnoT"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class PopularityAwareRecommender(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_users,\n",
        "        num_items,\n",
        "        user_texts,\n",
        "        item_texts,\n",
        "        embedding_dim=768,  # Set to 768 to match BERT output\n",
        "        lstm_hidden_dim=32,\n",
        "        gcn_hidden_dim=32,\n",
        "        num_gcn_layers=2,\n",
        "        temperature=0.07,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Initialize BERT model and tokenizer\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Use BERT to encode user and item embeddings\n",
        "        self.user_embeddings = self.encode_texts(user_texts)\n",
        "        self.item_embeddings = self.encode_texts(item_texts)\n",
        "\n",
        "        # LSTM for Sequential Modeling of Browsing History\n",
        "        self.user_lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            lstm_hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # LSTM for Item Popularity Time Series\n",
        "        self.popularity_lstm = nn.LSTM(\n",
        "            1,  # Input is the popularity score (time series)\n",
        "            lstm_hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # GCN layers for Item Embedding\n",
        "        self.gcn_layers = nn.ModuleList([\n",
        "            GCNConv(embedding_dim if i == 0 else gcn_hidden_dim, gcn_hidden_dim)\n",
        "            for i in range(num_gcn_layers)\n",
        "        ])\n",
        "\n",
        "        # Final projection layers\n",
        "        self.item_projection = nn.Linear(\n",
        "            gcn_hidden_dim + lstm_hidden_dim * 2,  # GCN output + popularity context\n",
        "            embedding_dim\n",
        "        )\n",
        "        self.user_projection = nn.Linear(\n",
        "            embedding_dim + lstm_hidden_dim * 2,  # Text embedding + interaction context\n",
        "            embedding_dim\n",
        "        )\n",
        "\n",
        "    def encode_texts(self, texts):\n",
        "        # Encode texts using BERT\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert_model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embeddings\n",
        "        return embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        user_ids,\n",
        "        interaction_histories,\n",
        "        item_popularity_seq,\n",
        "        edge_index\n",
        "    ):\n",
        "        # Get user embeddings from precomputed BERT embeddings\n",
        "        user_emb = self.user_embeddings[user_ids]  # Shape: [num_users, embedding_dim]\n",
        "\n",
        "        # Process user interaction history to get sequential representation\n",
        "        item_seq_emb = self.item_embeddings[interaction_histories]  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "        seq_output, _ = self.user_lstm(item_seq_emb)\n",
        "        interaction_context = seq_output[:, -1, :]  # Use the last hidden state as user interaction context\n",
        "\n",
        "        # Process popularity time series for each item\n",
        "        popularity_scores = item_popularity_seq.unsqueeze(-1)  # Shape: [batch_size, seq_len, 1]\n",
        "        pop_output, _ = self.popularity_lstm(popularity_scores)\n",
        "        pop_context = pop_output[:, -1, :]  # Last hidden state for popularity context\n",
        "\n",
        "        # Apply GCN layers to item embeddings\n",
        "        x = self.item_embeddings  # Initial node features\n",
        "        for gcn_layer in self.gcn_layers:\n",
        "            x = gcn_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=0.1, training=self.training)\n",
        "\n",
        "        # Combine item embeddings with popularity context\n",
        "        item_final = self.item_projection(\n",
        "            torch.cat([x, pop_context], dim=1)\n",
        "        )\n",
        "        user_final = self.user_projection(\n",
        "            torch.cat([user_emb, interaction_context], dim=1)\n",
        "        )\n",
        "\n",
        "        return user_final, item_final\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        user_emb,\n",
        "        item_emb,\n",
        "        positive_items,\n",
        "        negative_items,\n",
        "        popularity_weights\n",
        "    ):\n",
        "        # Compute similarity scores\n",
        "        pos_scores = torch.sum(user_emb * item_emb[positive_items], dim=1)\n",
        "        neg_scores = torch.sum(user_emb.unsqueeze(1) * item_emb[negative_items], dim=2)\n",
        "\n",
        "        # Apply popularity adjustment\n",
        "        pop_weights = popularity_weights[positive_items]\n",
        "        adjusted_pos_scores = pos_scores / (pop_weights + 1e-6)\n",
        "\n",
        "        # Compute InfoNCE loss with popularity adjustment\n",
        "        pos_exp = torch.exp(adjusted_pos_scores / self.temperature)\n",
        "        neg_exp = torch.exp(neg_scores / self.temperature).sum(dim=1)\n",
        "        loss = -torch.log(pos_exp / (pos_exp + neg_exp)).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_recommendations(self, user_emb, item_emb, k=10):\n",
        "        # Compute similarity scores\n",
        "        scores = torch.matmul(user_emb, item_emb.t())\n",
        "\n",
        "        # Get top-k recommendations\n",
        "        _, top_items = torch.topk(scores, k=k, dim=1)\n",
        "        return top_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIVx_CIkrGHa"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JYMHNYPuVrBb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Sample user profiles and movie descriptions\n",
        "user_profiles = [\n",
        "    \"User interested in action and adventure movies.\",\n",
        "    \"User prefers romantic comedies and dramas.\",\n",
        "    \"User enjoys science fiction and fantasy films.\"\n",
        "]\n",
        "\n",
        "movie_descriptions = [\n",
        "    \"An action-packed adventure in the mountains.\",\n",
        "    \"A heartwarming romantic comedy set in Paris.\",\n",
        "    \"A sci-fi epic exploring distant galaxies.\",\n",
        "    \"A fantasy tale of magic and dragons.\",\n",
        "    \"A drama about family and relationships.\"\n",
        "]\n",
        "\n",
        "# Simulated user interaction histories (indices of movies)\n",
        "user_interactions = [\n",
        "    [0, 1, 2],  # User 0 watched movies 0, 1, 2\n",
        "    [1, 3, 4],  # User 1 watched movies 1, 3, 4\n",
        "    [2, 3, 0]   # User 2 watched movies 2, 3, 0\n",
        "]\n",
        "\n",
        "# Simulated item popularity sequences (random values)\n",
        "item_popularity_seq = np.random.rand(len(movie_descriptions), 10)  # 10 time steps\n",
        "\n",
        "# Number of users and items\n",
        "num_users = len(user_profiles)\n",
        "num_items = len(movie_descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GqrjwaZYtlAf"
      },
      "outputs": [],
      "source": [
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, user_interactions, item_popularity_seq, num_users, num_items):\n",
        "        self.user_interactions = user_interactions\n",
        "        self.item_popularity_seq = item_popularity_seq\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.edge_index = self.build_edge_index()  # Automatically build edge index\n",
        "\n",
        "    def build_edge_index(self):\n",
        "        edges = []\n",
        "        for user_id, interactions in enumerate(self.user_interactions):\n",
        "            for item_id in interactions:\n",
        "                edges.append([user_id, self.num_users + item_id])  # user-to-item\n",
        "                edges.append([self.num_users + item_id, user_id])  # item-to-user (reverse)\n",
        "        return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_interaction = self.user_interactions[idx]\n",
        "        popularity_seq = self.item_popularity_seq[user_interaction]\n",
        "        return {\n",
        "            'user_id': idx,\n",
        "            'interaction_history': torch.tensor(user_interaction, dtype=torch.long),\n",
        "            'popularity_seq': torch.tensor(popularity_seq, dtype=torch.float),\n",
        "            'edge_index': self.edge_index  # Include edge_index\n",
        "        }\n",
        "# Create dataset and dataloader\n",
        "dataset = ToyDataset(user_interactions, item_popularity_seq, num_users, num_items)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPTJG4hvrILZ"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jgfkIetItj1g"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, num_epochs=5, learning_rate=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            user_ids = batch['user_id']\n",
        "            interaction_histories = batch['interaction_history']\n",
        "            popularity_seq = batch['popularity_seq']\n",
        "            edge_index = batch['edge_index']  # Get edge_index from the batch\n",
        "\n",
        "            # Forward pass\n",
        "            user_emb, item_emb = model(\n",
        "                user_ids=user_ids,\n",
        "                interaction_histories=interaction_histories,\n",
        "                item_popularity_seq=popularity_seq,\n",
        "                edge_index=edge_index\n",
        "            )\n",
        "\n",
        "            # Simulate positive and negative items for loss computation\n",
        "            positive_items = interaction_histories[:, -1]  # Last item in history as positive\n",
        "            negative_items = torch.randint(0, num_items, (len(user_ids), 5))  # 5 negative samples\n",
        "\n",
        "            # Compute popularity weights (for simplicity, using random values)\n",
        "            popularity_weights = torch.rand(num_items)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model.compute_loss(\n",
        "                user_emb=user_emb,\n",
        "                item_emb=item_emb,\n",
        "                positive_items=positive_items,\n",
        "                negative_items=negative_items,\n",
        "                popularity_weights=popularity_weights\n",
        "            )\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "coEUzPD_V0c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the model\n",
        "model = PopularityAwareRecommender(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    user_texts=user_profiles,\n",
        "    item_texts=movie_descriptions\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, dataloader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
